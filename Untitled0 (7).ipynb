{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOcDbyCI7Wr4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Assignment 1\n",
        "\n",
        "Ans1:- A model parameter is a configuration variable that is internal to the model and whose value can be estimated from the given data. They are required by the model when making predictions. Their values define the skill of the model on your problem.\n",
        "\n",
        "Ans2:- Correlation analysis determines the strength of a relationship between two item sets, which can be a dependent and an independent variable or even two independent variables.\n",
        "\n",
        "Ans3:- Machine learning is a branch of artificial intelligence focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.\n",
        "\n",
        "Ans4:- Loss is a numerical metric that describes how wrong a model's predictions are. Loss measures the distance between the model's predictions and the actual labels. The goal of training a model is to minimize the loss, reducing it to its lowest possible value.\n",
        "\n",
        "Ans5:- Categorical variables, aka discrete variables. These come in only a fixed number of values – like dead/alive, obese/overweight/normal/underweight, Apgar score. Continuous variables. These can have any value between a theoretical minimum and maximum, like birth weight, BMI, temperature, neutrophil count.\n",
        "\n",
        "Ans6:- One-hot encoding, ordinal encoding, and embedding are some of the most popular methods for handling categorical data in machine learning. Each of these methods has its own strengths and weaknesses, and the best approach will depend on the specific problem and dataset.\n",
        "\n",
        "Ans7:- Training data is used to train the machine-learning model. The more training data the model has, the better it can make predictions. Testing Data is used to evaluate the performance of the model. Exposure. The model can learn from the training data and improve its predictions.\n",
        "\n",
        "Ans8:- The sklearn. preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n",
        "\n",
        "Ans9:- Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set.\n",
        "\n",
        "Ans10:- To perform a train-test split, use libraries like scikit-learn in Python. Import the `train_test_split` function, specify the dataset, and set the test size (e.g., 20%). This function randomly divides the data into training and testing sets, preserving the distribution of classes or outcomes.\n",
        "\n",
        "Ans11:- Before fitting any model, it is often important to conduct an exploratory data analysis (EDA) in order to check assumptions, inspect the data for anomalies (such as missing, duplicated, or mis-coded data), and inform feature selection/transformation.\n",
        "\n",
        "Ans12:- Correlation analysis determines the strength of a relationship between two item sets, which can be a dependent and an independent variable or even two independent variables\n",
        "\n",
        "Ans13:- A negative correlation is a relationship between two variables that move in opposite directions. In other words, when variable A increases, variable B decreases. A negative correlation is also known as an inverse correlation. Two variables can have varying strengths of negative correlation.\n",
        "\n",
        "Ans14:- To calculate correlation in Python, you can use the pandas library, specifically the corr() method for DataFrames. It returns a correlation matrix showing the relationships between variables.\n",
        "\n",
        "Ans15:- Theoretically, the difference between the two types of relationships are easy to identify — an action or occurrence can cause another (e.g. smoking causes an increase in the risk of developing lung cancer), or it can correlate with another (e.g. smoking is correlated with alcoholism, but it does not cause alcoholism).\n",
        "\n",
        "Ans16:- Optimizers are algorithms used to find the optimal set of parameters for a model during the training process. These algorithms adjust the weights and biases in the model iteratively until they converge on a minimum loss value.\n",
        "\n",
        "Ans17:- linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models. The term linear model implies that the model is specified as a linear combination of features.\n",
        "\n",
        "Ans18:- Training: The fit() method adjusts the model parameters based on the input data ( X ) and the target values ( y ). Optimization: The model tries to minimize the error between its predictions and the actual target values.\n",
        "\n",
        "Ans19:- Model. predict passes the input vector through the model and returns the output tensor for each datapoint. Since the last layer in your model is a single Dense neuron, the output for any datapoint is a single value. And since you didn't specify an activation for the last layer, it will default to linear activation.\n",
        "\n",
        "Ans20:- Categorical variables, aka discrete variables. These come in only a fixed number of values – like dead/alive, obese/overweight/normal/underweight, Apgar score. Continuous variables. These can have any value between a theoretical minimum and maximum, like birth weight, BMI, temperature, neutrophil count.\n",
        "\n",
        "Ans21:- Feature scaling in Machine Learning is a method used to normalize the range of independent variables or features of data. Gradient descent and distance-based algorithms are heavily impacted by the range of features. Standardization and normalization are two primary ways to apply feature scaling in Machine Learning\n",
        "\n",
        "Ans22:- In min-max you will subtract the minimum value in the dataset with all the values and then divide this by the range of the dataset(maximum-minimum). In this case, your dataset will lie between 0 and 1 in all cases whereas in the previous case, it was between -1 and +1.\n",
        "\n",
        "Ans23:- The sklearn. preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n",
        "\n",
        "Ans24:- To perform a train-test split, use libraries like scikit-learn in Python. Import the `train_test_split` function, specify the dataset, and set the test size (e.g., 20%). This function randomly divides the data into training and testing sets, preserving the distribution of classes or outcomes.\n",
        "\n",
        "Ans25:- Encoding is the process of converting the data or a given sequence of characters, symbols, alphabets etc., into a specified format, for the secured transmission of data. Decoding is the reverse process of encoding which is to extract the information from the converted format.\n"
      ],
      "metadata": {
        "id": "2bX6As207dDh"
      }
    }
  ]
}